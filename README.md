ğŸŒ Transformer-Based English-Assamese Machine Translation

<div align="center">

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Transformers](https://img.shields.io/badge/%F0%9F%A4%97-Transformers-yellow.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

*A state-of-the-art neural machine translation system using Transformer architecture for high-quality English to Assamese translation.*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Performance](#-performance) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Highlights](#-key-highlights)
- [Features](#-features)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Configuration](#-configuration)
- [Performance](#-performance)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Overview

This project implements a **Transformer-based neural machine translation system** specifically designed for English to Assamese translation. It leverages state-of-the-art transformer architecture with BERT encoder and custom decoder to achieve high-quality translations with excellent BLEU and ROUGE scores.

### Why This Project?

- ğŸš€ **Modern Architecture**: Uses cutting-edge Transformer technology
- ğŸ“ˆ **High Accuracy**: Significantly outperforms traditional dictionary-based approaches
- ğŸ› ï¸ **Easy to Use**: Simple commands to train, evaluate, and translate
- ğŸ”§ **Customizable**: Flexible configuration for different use cases

---

## ğŸŒŸ Key Highlights

| Feature | Description |
|---------|-------------|
| **ğŸ¤– Transformer Architecture** | BERT encoder + Transformer decoder for superior quality |
| **ğŸ“Š High Performance** | Achieves excellent BLEU scores (15-25+) |
| **ğŸ¯ Beam Search** | Advanced decoding for better translation quality |
| **ğŸ’¬ Interactive Mode** | Real-time translation interface |
| **ğŸ”„ Modular Design** | Clean, maintainable codebase |
| **ğŸš€ Production Ready** | Comprehensive evaluation and optimization |

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  Core Features
- âœ… BERT-based encoder
- âœ… Custom Transformer decoder
- âœ… SentencePiece tokenization
- âœ… Teacher forcing training
- âœ… Beam search decoding

</td>
<td width="50%">

### ğŸ”§ Utilities
- âœ… Comprehensive evaluation metrics
- âœ… Interactive translation CLI
- âœ… Progress tracking
- âœ… Model checkpointing
- âœ… Easy configuration

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    A[English Input] --> B[BERT Tokenizer]
    B --> C[BERT Encoder]
    C --> D[Transformer Decoder]
    D --> E[SentencePiece Decoder]
    E --> F[Assamese Output]
```

### Model Components

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Encoder** | BERT-base-multilingual | Contextual understanding of English |
| **Decoder** | Custom Transformer | Generate Assamese translation |
| **Tokenizer (EN)** | BERT WordPiece | English text tokenization |
| **Tokenizer (AS)** | SentencePiece | Assamese text tokenization |
| **Training** | Cross-entropy + Teacher forcing | Model optimization |
| **Inference** | Beam search (size=3) | High-quality decoding |

---

## ğŸ“ Project Structure

```
ML_PROJECT/
â”‚
â”œâ”€â”€ ğŸš€ run_pipeline.py              # Main pipeline orchestrator
â”œâ”€â”€ ğŸ“‹ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“– README.md                    # This file
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # Automation scripts
â”‚   â”œâ”€â”€ download_data.py            # Download dataset from HuggingFace
â”‚   â”œâ”€â”€ prepare_data.py             # Convert data to training format
â”‚   â”œâ”€â”€ train_transformer.py        # Train transformer model
â”‚   â””â”€â”€ interactive_translator.py  # Interactive translation interface
â”‚
â”œâ”€â”€ ğŸ“ src/                         # Source code
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py             # Configuration and paths
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ transformer_dataset.py # PyTorch dataset
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ transformer.py          # Model architecture
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ transformer_trainer.py # Training utilities
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ transformer_evaluate.py # Evaluation metrics
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Data storage
â”‚   â”œâ”€â”€ raw/                        # Original datasets
â”‚   â””â”€â”€ processed/                  # Preprocessed data
â”‚
â””â”€â”€ ğŸ“ models/                      # Model storage
    â””â”€â”€ transformer_model/          # Trained models and tokenizers
```

---

## ğŸ› ï¸ Installation

### Prerequisites

Before you begin, ensure you have:

- âœ… **Python 3.8+** installed
- âœ… **8GB+ RAM** (recommended)
- âœ… **GPU** (optional, but recommended for training)
- âœ… **Git** installed

### Step-by-Step Setup

#### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Harsh-Raj-Jordan/ML_PROJECT.git
cd ML_PROJECT
```

#### 2ï¸âƒ£ Create Virtual Environment

<details>
<summary><b>ğŸªŸ Windows (PowerShell)</b></summary>

```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
```

</details>

<details>
<summary><b>ğŸªŸ Windows (Command Prompt)</b></summary>

```cmd
python -m venv .venv
.venv\Scripts\activate.bat
```

</details>

<details>
<summary><b>ğŸ§ Linux / ğŸ macOS</b></summary>

```bash
python3 -m venv .venv
source .venv/bin/activate
```

</details>

#### 3ï¸âƒ£ Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 4ï¸âƒ£ Set Up HuggingFace Token

```bash
huggingface-cli login
```

> **Note**: Get your token from [HuggingFace Settings](https://huggingface.co/settings/tokens)

---

## ğŸš€ Usage

### Option 1: Complete Pipeline (Recommended)

Run everything with a single command:

```bash
python run_pipeline.py
```

This will:
1. ğŸ“¥ Download dataset (2-5 min)
2. ğŸ”„ Prepare data and tokenizers (1-2 min)
3. ğŸ¤– Train model (30 min - 2 hrs)
4. ğŸ§ª Evaluate performance (1-2 min)

---

### Option 2: Step-by-Step Execution

Run specific components as needed:

| Command | Action | Time | Description |
|---------|--------|------|-------------|
| `python run_pipeline.py download` | ğŸ“¥ Download | 2-5 min | Fetch dataset from HuggingFace |
| `python run_pipeline.py prepare` | ğŸ”„ Prepare | 1-2 min | Process data & build tokenizers |
| `python run_pipeline.py train` | ğŸ¤– Train | 30m-2h | Train the transformer model |
| `python run_pipeline.py evaluate` | ğŸ§ª Evaluate | 1-2 min | Test model performance |
| `python run_pipeline.py interactive` | ğŸ’¬ Translate | Instant | Live translation interface |

---

### Option 3: Direct Script Execution

For advanced users:

```bash
# Download dataset
python scripts/download_data.py

# Prepare data
python scripts/prepare_data.py

# Train model
python scripts/train_transformer.py

# Interactive translation
python scripts/interactive_translator.py
```

---

### ğŸ’¬ Interactive Translation

Start the interactive translator:

```bash
python run_pipeline.py interactive
```

Example session:

```
ğŸŒ Transformer-Based English-Assamese Translator
================================================

Enter English text (or 'quit' to exit): hello how are you
Translation: à¦¨à¦®à¦¸à§à¦•à¦¾à§° à¦†à¦ªà§à¦¨à¦¿ à¦•à§‡à¦¨à§‡ à¦†à¦›à§‡

Enter English text (or 'quit' to exit): where is the hospital
Translation: à¦šà¦¿à¦•à¦¿à§à¦¸à¦¾à¦²à¦¯à¦¼ à¦•'à¦¤ à¦†à¦›à§‡
```

---

## âš™ï¸ Configuration

### Model Settings

Edit `src/config/settings.py` to customize:

```python
TRANSFORMER_CONFIG = {
    'model_name': 'bert-base-multilingual-cased',
    'vocab_size': 16000,              # Assamese vocabulary size
    'batch_size': 16,                 # Reduce if GPU memory limited
    'num_epochs': 5,                  # More epochs = better quality
    'learning_rate': 5e-5,            # Adjust if training unstable
    'max_len': 128,                   # Maximum sequence length
    'decoder_layers': 4,              # Number of decoder layers
    'decoder_heads': 8,               # Number of attention heads
    'decoder_ff_dim': 2048,           # Feed-forward dimension
    'dropout': 0.1,                   # Dropout rate
    'beam_size': 3,                   # Beam search width
}
```

### Training Tips

| Parameter | Recommendation | Effect |
|-----------|---------------|--------|
| **batch_size** | 8-32 | Higher = faster but more memory |
| **num_epochs** | 5-10 | More = better quality (diminishing returns) |
| **learning_rate** | 1e-5 to 1e-4 | Lower = more stable training |
| **decoder_layers** | 4-6 | More = better capacity but slower |

---

## ğŸ“Š Performance

### Expected Results

| Metric | Score Range | Interpretation |
|--------|-------------|----------------|
| **BLEU** | 15-25+ | Translation quality (higher is better) |
| **ROUGE-1** | 0.4-0.6 | Word overlap with reference |
| **ROUGE-2** | 0.3-0.5 | Bigram overlap |
| **Training Loss** | < 2.0 | Model convergence |

### Sample Translations

| English Input | Assamese Output | Quality |
|---------------|----------------|---------|
| "hello how are you today" | "à¦¨à¦®à¦¸à§à¦•à¦¾à§° à¦†à¦ªà§à¦¨à¦¿ à¦†à¦œà¦¿ à¦•à§‡à¦¨à§‡ à¦†à¦›à§‡" | â­â­â­â­â­ |
| "where is the nearest hospital" | "à¦¸à§°à§à¦¬à¦¾à¦§à¦¿à¦• à¦šà¦¿à¦•à¦¿à§à¦¸à¦¾à¦²à¦¯à¦¼ à¦•'à¦¤ à¦†à¦›à§‡" | â­â­â­â­â­ |
| "thank you very much" | "à¦¬à¦¹à§à¦¤ à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦" | â­â­â­â­â­ |

---

## ğŸ› Troubleshooting

### Common Issues & Solutions

<details>
<summary><b>âŒ "HuggingFace token not found"</b></summary>

**Solution:**
```bash
huggingface-cli login
```
Then enter your token from [here](https://huggingface.co/settings/tokens)

</details>

<details>
<summary><b>âŒ "CUDA out of memory"</b></summary>

**Solutions:**
1. Reduce `batch_size` in `src/config/settings.py`
2. Use CPU instead: Set `device='cpu'`
3. Clear GPU cache:
   ```python
   import torch
   torch.cuda.empty_cache()
   ```

</details>

<details>
<summary><b>âŒ "Module not found"</b></summary>

**Solution:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

</details>

<details>
<summary><b>â±ï¸ Training is too slow</b></summary>

**Solutions:**
1. Enable GPU if available
2. Reduce `batch_size` or `max_len`
3. Use smaller dataset for testing
4. Reduce `num_epochs`

</details>

### Verification Commands

Check your installation:

```bash
# Check PyTorch and CUDA
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

# Check if data exists
python -c "from pathlib import Path; print('Data:', Path('data/raw/eng_asm.json').exists())"

# Check if model exists
python -c "from pathlib import Path; print('Model:', Path('models/transformer_model').exists())"
```

---

## ğŸ”® Future Enhancements

- [ ] Model quantization for faster inference
- [ ] Web interface for translation
- [ ] Batch translation of documents
- [ ] REST API deployment
- [ ] Multi-language support
- [ ] Domain-specific fine-tuning
- [ ] Mobile app integration

---

## ğŸ¤ Contributing

We welcome contributions! Here's how:

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Commit** your changes: `git commit -m 'Add amazing feature'`
4. **Push** to the branch: `git push origin feature/amazing-feature`
5. **Open** a Pull Request

### Development Setup

```bash
# Fork and clone
git clone https://github.com/your-username/ML_PROJECT.git
cd ML_PROJECT

# Install dependencies
pip install -r requirements.txt

# Make changes and test
python run_pipeline.py

# Submit PR
git add .
git commit -m "Your descriptive commit message"
git push origin feature/your-feature
```

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Author

**Harsh Raj Jordan**
- GitHub: [@Harsh-Raj-Jordan](https://github.com/Harsh-Raj-Jordan)
- Project Link: [ML_PROJECT](https://github.com/Harsh-Raj-Jordan/ML_PROJECT)

---

## ğŸ™ Acknowledgments

- [HuggingFace](https://huggingface.co/) for transformers library and datasets
- [Google Research](https://research.google/) for Transformer architecture
- [AI4Bharat](https://ai4bharat.org/) for the BPCC dataset
- [PyTorch](https://pytorch.org/) team for the deep learning framework

---

## ğŸ“ Support

Need help? Here's what to do:

1. ğŸ“– Check the [Troubleshooting](#-troubleshooting) section
2. ğŸ” Search [existing issues](https://github.com/Harsh-Raj-Jordan/ML_PROJECT/issues)
3. ğŸ†• Create a [new issue](https://github.com/Harsh-Raj-Jordan/ML_PROJECT/issues/new) with details

---

<div align="center">

Made with â¤ï¸ by Harsh Raj Jordan

[â¬† Back to Top](#-transformer-based-english-assamese-machine-translation)

</div>
